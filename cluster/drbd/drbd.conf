
global {
    usage-count no;
}

common {
    net {
        protocol C;
    }
    options {
        quorum majority;
        quorum-minimum-redundancy majority;
        on-no-quorum io-error;
    }
}

resource "db" {
  device minor 1;
  meta-disk internal;
  # Change the disk accordingly. Also change it in the section of each node if necessary.
  disk "/dev/sdb";


  options {
    # Only necessary for startup. Comment this out on all machines after cluster is initialized and
    # run `drbdadm adjust db` to enable quorum (as set as default on common section above).
    quorum off;
  }

  on "postgres-server-1" {
    node-id 1;
    address 192.168.56.2:7789;
  }
  on "postgres-server-2" {
    node-id 2;
    address 192.168.56.3:7789;
    # you can override the disk here if needed:
    # disk "/dev/other-disk"
  }
  # We don't use diskless tie breaker here as mentioned in
  # https://linbit.com/drbd-user-guide/drbd-guide-9_0-en/#s-configuring-quorum-tiebreaker,
  # since it will not auto recover if there are frequent failures between the 3 nodes that has disk,
  # which will create a scenario which is similar as discussed in the section
  # "Failed to Recover the Cluster When Only 1 Out of 3 Nodes is Lost" in
  # https://www.binwang.me/2024-12-02-PostgreSQL-High-Availability-Solutions-Part-1.html
  on "postgres-server-3" {
    node-id 3;
    address 192.168.56.4:7789;
    # likewise you can change the disk on this node here if needed
  }
  connection-mesh {
    hosts "postgres-server-1" "postgres-server-2" "postgres-server-3";
  }
}